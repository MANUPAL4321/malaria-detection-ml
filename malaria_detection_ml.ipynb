{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define paths\n",
        "dataset_path = \"/content/cell_images/cell_images\"\n",
        "batch_size = 32  # Load 32 images at a time to save RAM\n",
        "img_size = (128, 128)  # Resize to 128x128\n",
        "\n",
        "# Define ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Normalize pixel values\n",
        "\n",
        "# Training data generator\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "# Validation data generator\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Print dataset info\n",
        "print(\"Training batches:\", len(train_generator))\n",
        "print(\"Validation batches:\", len(val_generator))\n"
      ],
      "metadata": {
        "id": "soRz539f5sD8",
        "outputId": "5f574cb8-6f8e-4c65-c5ea-f8ff1ba72419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22048 images belonging to 2 classes.\n",
            "Found 5510 images belonging to 2 classes.\n",
            "Training batches: 689\n",
            "Validation batches: 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will extract features from both Parasitized and Uninfected images."
      ],
      "metadata": {
        "id": "ZvdAxmaD84ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the correct dataset path after extracting the zip\n",
        "dataset_path = \"/content/cell_images/cell_images\"\n",
        "\n",
        "# List the contents of the 'cell_images' folder\n",
        "print(\"Contents of '/content/cell_images/cell_images':\", os.listdir(dataset_path))\n",
        "\n",
        "# Correct the paths to Parasitized and Uninfected directories\n",
        "parasitized_dir = os.path.join(dataset_path, \"Parasitized\")\n",
        "uninfected_dir = os.path.join(dataset_path, \"Uninfected\")\n",
        "\n",
        "# Verify directory existence\n",
        "if not os.path.exists(parasitized_dir):\n",
        "    print(f\"Error: {parasitized_dir} does not exist.\")\n",
        "else:\n",
        "    print(f\"Found Parasitized Directory: {parasitized_dir}\")\n",
        "\n",
        "if not os.path.exists(uninfected_dir):\n",
        "    print(f\"Error: {uninfected_dir} does not exist.\")\n",
        "else:\n",
        "    print(f\"Found Uninfected Directory: {uninfected_dir}\")\n"
      ],
      "metadata": {
        "id": "aTAljFX49zR7",
        "outputId": "7a914e1e-5477-4c26-9ba6-65412151ff47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of '/content/cell_images/cell_images': ['Uninfected', 'Parasitized']\n",
            "Found Parasitized Directory: /content/cell_images/cell_images/Parasitized\n",
            "Found Uninfected Directory: /content/cell_images/cell_images/Uninfected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gc  # Garbage collection module\n",
        "\n",
        "IMG_SIZE = 64  # Reduce the image size to 64x64\n",
        "BATCH_SIZE = 50  # Reduce the batch size for testing\n",
        "\n",
        "# Preprocessing function with batch processing and memory management\n",
        "def load_and_preprocess_images(directory, label, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Get a list of image filenames, excluding unwanted files like Thumbs.db\n",
        "    valid_files = [f for f in os.listdir(directory) if f.lower() != \"thumbs.db\"]\n",
        "\n",
        "    # Process images in batches\n",
        "    for i, img_name in enumerate(valid_files):\n",
        "        if i % batch_size == 0 and i > 0:\n",
        "            # Yield current batch\n",
        "            yield np.array(images), np.array(labels)\n",
        "            images, labels = [], []  # Reset for next batch\n",
        "            gc.collect()  # Run garbage collection to free memory\n",
        "\n",
        "        img_path = os.path.join(directory, img_name)\n",
        "\n",
        "        # Load image in RGB format\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"Skipping corrupted file: {img_name}\")\n",
        "            continue\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize the image\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "        # Normalize the image (scaling pixel values to [0,1])\n",
        "        img = img / 255.0\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Yield last batch if any images remain\n",
        "    if images:\n",
        "        yield np.array(images), np.array(labels)\n",
        "    gc.collect()  # Final garbage collection\n",
        "\n",
        "# Initialize empty lists to store the data\n",
        "parasitized_images, parasitized_labels = [], []\n",
        "uninfected_images, uninfected_labels = [], []\n",
        "\n",
        "# Load Parasitized Images in batches\n",
        "for batch_images, batch_labels in load_and_preprocess_images(parasitized_dir, label=1):\n",
        "    parasitized_images.append(batch_images)\n",
        "    parasitized_labels.append(batch_labels)\n",
        "\n",
        "# Load Uninfected Images in batches\n",
        "for batch_images, batch_labels in load_and_preprocess_images(uninfected_dir, label=0):\n",
        "    uninfected_images.append(batch_images)\n",
        "    uninfected_labels.append(batch_labels)\n",
        "\n",
        "# Concatenate all the batches into one array\n",
        "parasitized_images = np.concatenate(parasitized_images)\n",
        "parasitized_labels = np.concatenate(parasitized_labels)\n",
        "uninfected_images = np.concatenate(uninfected_images)\n",
        "uninfected_labels = np.concatenate(uninfected_labels)\n",
        "\n",
        "# Check the final shape of the loaded data\n",
        "print(f\"Loaded {len(parasitized_images)} Parasitized images\")\n",
        "print(f\"Loaded {len(uninfected_images)} Uninfected images\")\n"
      ],
      "metadata": {
        "id": "WU_bmb7oBppq",
        "outputId": "f51382bf-1f7b-413f-c483-46b9a9823c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13779 Parasitized images\n",
            "Loaded 13779 Uninfected images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "print(\"Available memory (in GB):\", psutil.virtual_memory().available / (1024**3))\n"
      ],
      "metadata": {
        "id": "HmQkqiSh_iPC",
        "outputId": "ddd787fe-f515-4471-f7ff-746535ddcb6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available memory (in GB): 0.9740028381347656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the images and labels into a single array\n",
        "images = np.concatenate([parasitized_images, uninfected_images], axis=0)\n",
        "labels = np.concatenate([parasitized_labels, uninfected_labels], axis=0)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "68T6hkS1_ztG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom CNN Models (2-Layer vs. 3-Layer)"
      ],
      "metadata": {
        "id": "nFQar9sVGDX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "W7hRIbvuCuVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_2_layer_cnn(input_shape=(128, 128, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # For binary classification (parasitized vs uninfected)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XTRPNPsuGGwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_3_layer_cnn(input_shape=(128, 128, 3)):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "EvsAqQYZGJ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Models"
      ],
      "metadata": {
        "id": "HAhZVu-cGSxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train shape before reshaping: {X_train.shape}\")\n",
        "print(f\"X_test shape before reshaping: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "UAEwLHGwHYSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cnn = X_train.reshape(-1, 128, 128, 3)\n",
        "X_test_cnn = X_test.reshape(-1, 128, 128, 3)\n"
      ],
      "metadata": {
        "id": "G3k15AkhHd48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOehtfaJGVc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}